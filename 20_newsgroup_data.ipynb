{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this funtion is to fetch all file name and put in to a list x \n",
    "# y value is corresponding to newgroup which vary from 0 to 19 to represent all 20 newsgroup\n",
    "def get_data(path):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    i=0\n",
    "    for newspaper in os.listdir(path):\n",
    "        n_path=path+'/'+newspaper\n",
    "        for filename in os.listdir(n_path):\n",
    "            new_path=n_path+'/'+filename\n",
    "            x.append(new_path)\n",
    "            y.append(i)\n",
    "        i+=1\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is to remove punctuation and then tokenize it \n",
    "def get_tokens(doc):\n",
    "        deletechars={ord(c): None for c in string.punctuation} # to remove punctuation\n",
    "        no_punctuation=doc.translate(deletechars)\n",
    "        tokens=nltk.word_tokenize(no_punctuation)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this funtion is to tokenize all document into a token_data and also put tokens for every \n",
    "# individual newsgroup into a seperate list \n",
    "def tokenization(x_train,y_train):\n",
    "    token_data=[]\n",
    "    token_list=[[] for i in range(20)]\n",
    "    for i in range(len(x_train)):\n",
    "        file=open(x_train[i],'r')\n",
    "        doc=file.read()\n",
    "        lowers=doc.lower()\n",
    "        token=get_tokens(lowers)\n",
    "        filter_token=[w for w in token if not w in stopset]\n",
    "        token_data.extend(filter_token)\n",
    "        token_list[y_train[i]].extend(filter_token)\n",
    "    return token_data,token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this funtion is to create an 2d np array of 20 * 20001 to store frequency of feature words for individual newsgroup\n",
    "def get_dictionary(token_list,feature_name):\n",
    "    x=np.zeros((20,2000),dtype=int)\n",
    "    for i in range(20):\n",
    "        token_data=token_list[i]\n",
    "        for word in token_data:\n",
    "            if word in feature_name:\n",
    "                j=feature_name.index(word)\n",
    "                x[i][j]+=1\n",
    "    total=[]\n",
    "    for i in range(20):\n",
    "        sum=0\n",
    "        for j in range(2000):\n",
    "            sum+=x[i][j]\n",
    "        total.append(sum)\n",
    "    t_arr=np.array(total)\n",
    "    t_arr=t_arr.reshape(len(t_arr),1) \n",
    "    x=np.append(x,t_arr,axis=1)    # to add a column in array which store sum of all words for every individual newspaper group\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is to select top 2000 words from dictionary\n",
    "def get_feature_name(count):\n",
    "    feature_name=[word for (word, freq) in count.most_common(2000)]\n",
    "    return feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is to calculate probablity of word occurence with lapalce correction\n",
    "def probablity(dictionary,x,feature_name,current_class):\n",
    "    output=np.log(1)-np.log(20)\n",
    "    for word in x:\n",
    "        if word in feature_name:\n",
    "            index=feature_name.index(word)\n",
    "            temp=np.log(dictionary[current_class][index]+1)-np.log(dictionary[current_class][dictionary.shape[1]-1]+2000)\n",
    "            output+=temp\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is to tokenize word for a file \n",
    "def tokenize_word(path):\n",
    "    file=open(path,'r')\n",
    "    doc=file.read()\n",
    "    lowers=doc.lower()\n",
    "    token=get_tokens(lowers)\n",
    "    filter_token=[w for w in token if not w in stopset]\n",
    "    return filter_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is to provide y predicted value\n",
    "def predict(dictionary,feature_name,x_test):\n",
    "    y_pred=[]\n",
    "    for i in range(len(x_test)):\n",
    "        words=tokenize_word(x_test[i])\n",
    "        best_p=-1e9\n",
    "        best_class=-1\n",
    "        for j in range(20):\n",
    "            current=probablity(dictionary,words,feature_name,j)\n",
    "            if(current>best_p):\n",
    "                best_p=current\n",
    "                best_class=j \n",
    "        y_pred.append(best_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path='../datasets/20_newsgroups'\n",
    "x,y=get_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,shuffle=True,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_data,token_list=tokenization(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=Counter(token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name=get_feature_name(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=get_dictionary(token_list,feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=predict(dictionary,feature_name,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.73      0.71       237\n",
      "          1       0.78      0.76      0.77       237\n",
      "          2       0.83      0.84      0.84       271\n",
      "          3       0.87      0.81      0.84       247\n",
      "          4       0.85      0.90      0.87       253\n",
      "          5       0.95      0.86      0.91       237\n",
      "          6       0.74      0.89      0.81       247\n",
      "          7       0.83      0.92      0.87       254\n",
      "          8       0.85      0.92      0.89       259\n",
      "          9       0.83      0.92      0.87       244\n",
      "         10       0.94      0.81      0.87       251\n",
      "         11       0.95      0.86      0.90       242\n",
      "         12       0.80      0.88      0.84       269\n",
      "         13       0.89      0.87      0.88       256\n",
      "         14       0.92      0.88      0.90       262\n",
      "         15       0.93      0.98      0.96       260\n",
      "         16       0.76      0.88      0.81       253\n",
      "         17       0.94      0.82      0.87       251\n",
      "         18       0.71      0.58      0.64       242\n",
      "         19       0.59      0.50      0.54       228\n",
      "\n",
      "avg / total       0.83      0.83      0.83      5000\n",
      "\n",
      "[[173   1   0   0   0   0   0   3   3   0   1   3   2   2   2   2   4   1\n",
      "    3  37]\n",
      " [  1 179  10   5   8   4   4   1   5   0   1   1   9   4   3   0   0   1\n",
      "    0   1]\n",
      " [  0   9 227  12   6   1   1   1   2   0   3   0   5   0   1   0   0   0\n",
      "    0   3]\n",
      " [  0   5  11 201   7   2  10   3   0   0   0   0   7   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   4   7 227   0   8   1   0   0   0   0   2   2   0   0   0   0\n",
      "    1   0]\n",
      " [  0  10  11   0   3 205   4   0   1   0   0   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   3   3   2   0 219   6   2   1   0   0   5   2   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   8 233   3   0   0   1   6   0   0   0   2   0\n",
      "    0   0]\n",
      " [  0   3   0   0   1   0   7   8 239   0   0   0   0   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   1   0   6   0   4 225   7   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   2   1   4  37 203   0   1   0   0   0   0   0\n",
      "    2   0]\n",
      " [  0   3   1   0   2   3   3   1   1   0   0 208   7   1   0   0   5   0\n",
      "    6   1]\n",
      " [  0   4   1   2   6   0   5   9   2   1   0   1 236   1   0   0   1   0\n",
      "    0   0]\n",
      " [  7   4   2   0   0   0   5   2   4   0   0   0   3 222   3   0   1   0\n",
      "    0   3]\n",
      " [  1   6   1   0   2   0   2   3   4   1   1   0   1   2 230   0   2   0\n",
      "    2   4]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   1   1 255   0   1\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   3   2   0   2   0   1   0   0   0   0 223   0\n",
      "   12  10]\n",
      " [  6   1   0   0   0   0   3   3   4   2   0   1   2   2   2   2   2 205\n",
      "   14   2]\n",
      " [  6   0   0   1   0   0   3   3   3   2   0   4   2   5   4   0  39  10\n",
      "  140  20]\n",
      " [ 55   0   1   0   0   0   2   2   0   0   0   0   1   4   2  14  16   1\n",
      "   15 115]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
